{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* **Fine-tuning dataset**: \n",
    "    - The IMDb dataset is a well-known benchmark dataset for sentiment analysis tasks, containing movie reviews labeled as positive or negative.\n",
    "    - It's lightweight in terms of data size, containing a relatively small number of samples compared to other datasets like Common Crawl or Wikipedia.\n",
    "    - Training on the IMDb dataset requires less computational resources and time compared to larger datasets, making it suitable for lightweight fine-tuning projects.\n",
    "    - The IMDb dataset provides a good balance between simplicity and complexity, making it ideal for initial experimentation and prototyping of NLP models.\n",
    "    - Since it focuses on sentiment analysis, the task is straightforward and can be easily adapted for various downstream applications such as review classification or sentiment detection in social media posts.\n",
    "    - Availability: The IMDb dataset is widely used and readily accessible, making it convenient for researchers and practitioners to obtain and work with for fine-tuning projects.\n",
    "    - The IMDb dataset often serves as a starting point for fine-tuning pre-trained language models like BERT or DistilBERT for sentiment analysis tasks, allowing for efficient transfer learning.\n",
    "\n",
    "* **Model**: \n",
    "    - DistilBERT-base-uncased is a pre-trained transformer-based model specifically fine-tuned for natural language processing tasks.\n",
    "    - It is trained on a large corpus of text data, which includes a diverse range of language patterns and nuances, making it well-suited for understanding the complexities of human language.\n",
    "    - DistilBERT-base-uncased is a distilled version of BERT, which means it retains much of the performance of BERT while being smaller and faster, making it more practical for deployment and inference in real-world applications.\n",
    "    - The \"uncased\" variant of DistilBERT means that it does not differentiate between uppercase and lowercase letters, which is appropriate for text classification tasks where the case of the letters may not carry significant semantic meaning.\n",
    "    - Given the nature of the IMDB dataset, which consists of movie reviews that are primarily text-based, DistilBERT's ability to capture contextual information and semantic meaning from the text is particularly advantageous for sentiment analysis tasks such as classifying reviews as positive or negative.\n",
    "    - DistilBERT has been widely adopted and benchmarked in various natural language processing tasks, including sentiment analysis, achieving competitive performance with relatively lower computational resources compared to larger models like BERT.\n",
    "\n",
    "* **Evaluation approach**:\n",
    "    - Accuracy is a straightforward and intuitive metric that measures the overall correctness of the classification model.\n",
    "    - For sentiment analysis tasks like classifying IMDb reviews as positive or negative, accuracy provides a clear indication of how well the model performs in correctly predicting the sentiment of the reviews.\n",
    "    - The IMDb dataset is balanced, meaning it contains roughly equal numbers of positive and negative reviews. In such cases, accuracy is a suitable metric because it reflects the model's ability to correctly classify both positive and negative instances.\n",
    "    - Accuracy is easy to interpret and communicate, making it accessible to stakeholders and non-technical audiences.\n",
    "    - Since the goal of sentiment analysis is to accurately determine the sentiment expressed in text data, accuracy aligns well with the primary objective of the task.\n",
    "    - While accuracy may not be the only metric to consider (other metrics like precision, recall, and F1 score can provide additional insights, especially in imbalanced datasets), it serves as a fundamental measure of model performance and is often used as a baseline metric for classification tasks.\n",
    "\n",
    "* **PEFT technique**:\n",
    "    - Low-rank adaptation is a technique used to adapt large pre-trained language models, like DistilBERT, to specific downstream tasks while reducing computational costs and memory requirements.\n",
    "    - By applying low-rank adaptation, we aim to fine-tune the parameters of DistilBERT in a low-dimensional subspace, which helps in retaining the essential information captured during pre-training while adapting the model to the IMDb sentiment classification task.\n",
    "    - Low-rank adaptation helps mitigate overfitting by reducing the dimensionality of the parameter space, thereby enhancing the generalization capability of the model to unseen data.\n",
    "    - The IMDb sentiment classification task typically involves a binary classification problem (positive or negative sentiment), making it suitable for low-rank adaptation as it allows us to focus the model's capacity on relevant features for sentiment analysis.\n",
    "    - Low-rank adaptation facilitates faster convergence during fine-tuning, enabling us to efficiently leverage the knowledge captured by DistilBERT on the IMDb dataset and achieve competitive performance with fewer computational resources.\n",
    "    - Through low-rank adaptation, we strike a balance between model complexity and task-specific performance, making it a practical and effective technique for sentiment analysis tasks like classifying IMDb reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badfb5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "\n",
    "from peft import (AutoPeftModelForSequenceClassification, LoraConfig, TaskType,\n",
    "                  get_peft_model)\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0615c304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 2, 'test': 2, 'unsupervised': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show dataset contained\n",
    "dataset.num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d2e1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show dataset details\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT Value(dtype='string', id=None)\n",
      "LABEL ClassLabel(names=['neg', 'pos'], id=None)\n"
     ]
    }
   ],
   "source": [
    "# show training set text and labels\n",
    "print(\"TEXT\", dataset[\"train\"].features[f\"text\"])\n",
    "print(\"LABEL\", dataset[\"train\"].features[f\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "LABEL: 0\n"
     ]
    }
   ],
   "source": [
    "# show example test set text and labels\n",
    "print(\"TEXT:\", dataset[\"test\"][\"text\"][0])\n",
    "print(\"LABEL:\", dataset[\"test\"][\"label\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up labels, label ids and number of labels\n",
    "labels = dataset[\"train\"].features[f\"label\"].names\n",
    "\n",
    "id2label = {i: name for i, name in enumerate(labels)}\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "\n",
    "\n",
    "label_count = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tokenizer and paddiung\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "if not tokenizer.pad_token:\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a931cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"neg\",\n",
      "    \"1\": \"pos\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"neg\": 0,\n",
      "    \"pos\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# set up model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=label_count, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "\n",
    "print(model)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d97e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW INPUT: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "TOKENIZED OUTPUT: ['[CLS]', 'i', 'rented', 'i', 'am', 'curious', '-', 'yellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967', '.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u', '.', 's', '.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', '\"', 'controversial', '\"', 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myself', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attention', '##s', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'sw', '##ede', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'vietnam', 'war', 'and', 'race', 'issues', 'in', 'the', 'united', 'states', '.', 'in', 'between', 'asking', 'politicians', 'and', 'ordinary', 'den', '##ize', '##ns', 'of', 'stockholm', 'about', 'their', 'opinions', 'on', 'politics', ',', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', ',', 'classmates', ',', 'and', 'married', 'men', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'what', 'kills', 'me', 'about', 'i', 'am', 'curious', '-', 'yellow', 'is', 'that', '40', 'years', 'ago', ',', 'this', 'was', 'considered', 'pornographic', '.', 'really', ',', 'the', 'sex', 'and', 'nu', '##dity', 'scenes', 'are', 'few', 'and', 'far', 'between', ',', 'even', 'then', 'it', \"'\", 's', 'not', 'shot', 'like', 'some', 'cheap', '##ly', 'made', 'porn', '##o', '.', 'while', 'my', 'country', '##men', 'mind', 'find', 'it', 'shocking', ',', 'in', 'reality', 'sex', 'and', 'nu', '##dity', 'are', 'a', 'major', 'staple', 'in', 'swedish', 'cinema', '.', 'even', 'ing', '##mar', 'bergman', ',', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', ',', 'had', 'sex', 'scenes', 'in', 'his', 'films', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'i', 'do', 'com', '##men', '##d', 'the', 'filmmakers', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theaters', 'in', 'america', '.', 'i', 'am', 'curious', '-', 'yellow', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potatoes', '(', 'no', 'pun', 'intended', ')', 'of', 'swedish', 'cinema', '.', 'but', 'really', ',', 'this', 'film', 'doesn', \"'\", 't', 'have', 'much', 'of', 'a', 'plot', '.', '[SEP]']\n",
      "TOKEN_IDS: [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 143, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 216, 217, 218, 218, 219, 220, 221, 222, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 272, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, None]\n"
     ]
    }
   ],
   "source": [
    "# tokenize example text\n",
    "tokenized_input = tokenizer(dataset[\"train\"][0][\"text\"], truncation=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "print(\"RAW INPUT:\", dataset[\"train\"][0][\"text\"])\n",
    "print(\"TOKENIZED OUTPUT:\", tokens)\n",
    "print(\"TOKEN_IDS:\", tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d0e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([CLS],None), (i,0), (rented,1), (i,2), (am,3), (curious,4), (-,5), (yellow,6), (from,7), (my,8), (video,9), (store,10), (because,11), (of,12), (all,13), (the,14), (controversy,15), (that,16), (surrounded,17), (it,18), (when,19), (it,20), (was,21), (first,22), (released,23), (in,24), (1967,25), (.,26), (i,27), (also,28), (heard,29), (that,30), (at,31), (first,32), (it,33), (was,34), (seized,35), (by,36), (u,37), (.,38), (s,39), (.,40), (customs,41), (if,42), (it,43), (ever,44), (tried,45), (to,46), (enter,47), (this,48), (country,49), (,,50), (therefore,51), (being,52), (a,53), (fan,54), (of,55), (films,56), (considered,57), (\",58), (controversial,59), (\",60), (i,61), (really,62), (had,63), (to,64), (see,65), (this,66), (for,67), (myself,68), (.,69), (<,70), (br,71), (/,72), (>,73), (<,74), (br,75), (/,76), (>,77), (the,78), (plot,79), (is,80), (centered,81), (around,82), (a,83), (young,84), (swedish,85), (drama,86), (student,87), (named,88), (lena,89), (who,90), (wants,91), (to,92), (learn,93), (everything,94), (she,95), (can,96), (about,97), (life,98), (.,99), (in,100), (particular,101), (she,102), (wants,103), (to,104), (focus,105), (her,106), (attention,107), (##s,107), (to,108), (making,109), (some,110), (sort,111), (of,112), (documentary,113), (on,114), (what,115), (the,116), (average,117), (sw,118), (##ede,118), (thought,119), (about,120), (certain,121), (political,122), (issues,123), (such,124), (as,125), (the,126), (vietnam,127), (war,128), (and,129), (race,130), (issues,131), (in,132), (the,133), (united,134), (states,135), (.,136), (in,137), (between,138), (asking,139), (politicians,140), (and,141), (ordinary,142), (den,143), (##ize,143), (##ns,143), (of,144), (stockholm,145), (about,146), (their,147), (opinions,148), (on,149), (politics,150), (,,151), (she,152), (has,153), (sex,154), (with,155), (her,156), (drama,157), (teacher,158), (,,159), (classmates,160), (,,161), (and,162), (married,163), (men,164), (.,165), (<,166), (br,167), (/,168), (>,169), (<,170), (br,171), (/,172), (>,173), (what,174), (kills,175), (me,176), (about,177), (i,178), (am,179), (curious,180), (-,181), (yellow,182), (is,183), (that,184), (40,185), (years,186), (ago,187), (,,188), (this,189), (was,190), (considered,191), (pornographic,192), (.,193), (really,194), (,,195), (the,196), (sex,197), (and,198), (nu,199), (##dity,199), (scenes,200), (are,201), (few,202), (and,203), (far,204), (between,205), (,,206), (even,207), (then,208), (it,209), (',210), (s,211), (not,212), (shot,213), (like,214), (some,215), (cheap,216), (##ly,216), (made,217), (porn,218), (##o,218), (.,219), (while,220), (my,221), (country,222), (##men,222), (mind,223), (find,224), (it,225), (shocking,226), (,,227), (in,228), (reality,229), (sex,230), (and,231), (nu,232), (##dity,232), (are,233), (a,234), (major,235), (staple,236), (in,237), (swedish,238), (cinema,239), (.,240), (even,241), (ing,242), (##mar,242), (bergman,243), (,,244), (arguably,245), (their,246), (answer,247), (to,248), (good,249), (old,250), (boy,251), (john,252), (ford,253), (,,254), (had,255), (sex,256), (scenes,257), (in,258), (his,259), (films,260), (.,261), (<,262), (br,263), (/,264), (>,265), (<,266), (br,267), (/,268), (>,269), (i,270), (do,271), (com,272), (##men,272), (##d,272), (the,273), (filmmakers,274), (for,275), (the,276), (fact,277), (that,278), (any,279), (sex,280), (shown,281), (in,282), (the,283), (film,284), (is,285), (shown,286), (for,287), (artistic,288), (purposes,289), (rather,290), (than,291), (just,292), (to,293), (shock,294), (people,295), (and,296), (make,297), (money,298), (to,299), (be,300), (shown,301), (in,302), (pornographic,303), (theaters,304), (in,305), (america,306), (.,307), (i,308), (am,309), (curious,310), (-,311), (yellow,312), (is,313), (a,314), (good,315), (film,316), (for,317), (anyone,318), (wanting,319), (to,320), (study,321), (the,322), (meat,323), (and,324), (potatoes,325), ((,326), (no,327), (pun,328), (intended,329), (),330), (of,331), (swedish,332), (cinema,333), (.,334), (but,335), (really,336), (,,337), (this,338), (film,339), (doesn,340), (',341), (t,342), (have,343), (much,344), (of,345), (a,346), (plot,347), (.,348), ([SEP],None), "
     ]
    }
   ],
   "source": [
    "# show tokenized example text\n",
    "for token, word_id in zip(tokens, tokenized_input.word_ids()):\n",
    "\n",
    "    print(f\"({token},{word_id})\", end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b09a75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tokenizer for corpus\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_input = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "748071e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "0\n",
      "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# check tokenizer\n",
    "print(tokenized_input[\"train\"][0][\"text\"])\n",
    "print(tokenized_input[\"train\"][0][\"label\"])\n",
    "print(tokenized_input[\"train\"][0][\"input_ids\"])\n",
    "print(tokenized_input[\"train\"][0][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the pad token ID from the tokenizer to the pad_token_id attribute of the model's configuration.\n",
    "# This ensures consistency between the tokenizer and the model during tokenization and padding operations.\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code iterates over all parameters of the base model within a larger neural network model (presumably a pre-trained model).\n",
    "# It sets the requires_grad attribute of each parameter to False, effectively freezing them from being updated during the training process.\n",
    "# This is necessary when fine-tuning a pre-trained model where we want to keep the parameters of the base model fixed while only updating the parameters of the added layers or the head of the model.\n",
    "# By setting requires_grad to False, we prevent gradients from being computed and accumulated for these parameters during backpropagation, thus ensuring that they remain unchanged.\n",
    "for param in model.base_model.parameters():\n",
    "\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of class labels\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up accuracy as metric function\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be82ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training function\n",
    "def train_model(model, output_dir, train_dataset, eval_dataset, tokenizer, compute_metrics):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=32,\n",
    "            per_device_eval_batch_size=32,\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "        ),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a54913e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93910448b844960bce1e1faa19bae5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6288, 'grad_norm': 0.7739212512969971, 'learning_rate': 7.21227621483376e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16adc5f9698b4a2c897f98607957a127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5526282787322998, 'eval_accuracy': 0.80684, 'eval_runtime': 114.0632, 'eval_samples_per_second': 219.177, 'eval_steps_per_second': 6.856, 'epoch': 1.0}\n",
      "{'train_runtime': 232.9497, 'train_samples_per_second': 107.319, 'train_steps_per_second': 3.357, 'train_loss': 0.6069739456371883, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# train foundation model on training data\n",
    "trainer_foundation = train_model(\n",
    "    model, './foundation_model', tokenized_input[\"train\"], tokenized_input[\"test\"], tokenizer, compute_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5265c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b2a85c29534ea2b1946d71a0b67e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5526282787322998,\n",
       " 'eval_accuracy': 0.80684,\n",
       " 'eval_runtime': 115.7068,\n",
       " 'eval_samples_per_second': 216.063,\n",
       " 'eval_steps_per_second': 6.758,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate foundation model on test data\n",
    "trainer_foundation.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02f757",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 813,314 || all params: 67,768,324 || trainable%: 1.2001388731407907\n"
     ]
    }
   ],
   "source": [
    "# set up LoRA (low-Rank Adaption)\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, target_modules=[\n",
    "                         'q_lin', 'k_lin', 'v_lin'], inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "lora_model = get_peft_model(model, peft_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d3bf453b474e21ad51f7b59cebff70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3667, 'grad_norm': 1.7551478147506714, 'learning_rate': 7.21227621483376e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a060d8acfe46fdabf0945da181e3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2818361818790436, 'eval_accuracy': 0.88072, 'eval_runtime': 118.1663, 'eval_samples_per_second': 211.566, 'eval_steps_per_second': 6.618, 'epoch': 1.0}\n",
      "{'train_runtime': 379.7227, 'train_samples_per_second': 65.838, 'train_steps_per_second': 2.059, 'train_loss': 0.3405481616554358, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# train LoRA-finetuned model of training data\n",
    "trainer_finetuning = train_model(\n",
    "    lora_model, './lora_model', tokenized_input[\"train\"], tokenized_input[\"test\"], tokenizer, compute_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d775766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb664c1a6144276886be8e55dfe0d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2818361818790436,\n",
       " 'eval_accuracy': 0.88072,\n",
       " 'eval_runtime': 116.7894,\n",
       " 'eval_samples_per_second': 214.06,\n",
       " 'eval_steps_per_second': 6.696,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate LoRA-finetuned model of test data\n",
    "trainer_finetuning.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "lora_model.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f079b93",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d12e2363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load best model\n",
    "best_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"lora_model\",  num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd1e2c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): MultiHeadSelfAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (k_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract aim data\n",
    "infer_data = dataset[\"unsupervised\"][\"text\"][:5]\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "368bdf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)', 'When I say this is my favourite film of all time, that comment is not to be taken lightly. I probably watch far too many films than is healthy for me, and have loved quite a few of them. I first saw \"La Femme Nikita\" nearly ten years ago, and it still manages to be my absolute favourite. Why?<br /><br />This is more than an incredibly stylish and sexy thriller. Luc Besson\\'s great flair for impeccable direction, fashion, and appropriate usage of music makes this a very watchable film. But it is Anne Parillaud\\'s perfect rendering of a complex character who transforms from a heartless killer into a compassionate, vibrant young woman that makes this film beautiful. I can\\'t keep my eyes off of her when she is on screen.<br /><br />I have seen several of Luc Besson\\'s films including \"Subway\", \"The Professional\", and the irritating \"Fifth Element\", and \"Nikita\" is without a doubt, far superior to any of these. Although this film has tragic elements, it is ultimately extremely hopeful. It is the story of a person who is cruel and merciless, who ultimately comes to realize her own humanity and her own personal power. That, to me is extremely inspiring. If there is hope for Nikita, there is hope for all of us.', 'I saw this movie because I am a huge fan of the TV series of the same name starring Roy Dupuis and Pet Wilson. The movie was really good and I saw how the TV show is based on the movie. A few episodes of the TV series came directly from the movie and their similarity was amazing. To keep things short, any fan of the movie has to watch the series and any fan of the series must see the original Nikita.', \"Being that the only foreign films I usually like star a Japanese person in a rubber suit who crushes little tiny buildings and tanks, I had high hopes for this movie. I thought that this was a movie that wouldn't put me to sleep. WRONG! Starts off with a bang, okay, now she's in training, alright, she's an assassin, I'm still with you, oh, now she's having this moral dilemma and she can't decide if she loves her boyfriend or her controller, zzzzz.... Oh well, back to Gamera!\", \"After seeing Point of No Return (a great movie) and being told that the original was better, I was certainly thrilled to see that one of the indie film channels was running La Femme Nikita. Then I saw the movie. Ouch! This was a major let-down.<br /><br />Nikita herself reminds me of Jar Jar Binks more than any other character I've seen recently. She comes across entirely as comic relief. The movie simply has nothing to recommend it besides the core concept of an evil, inhuman character paradoxically learning to be human while training as an assassin, and that concept failed miserably in Nikita due to the poor writing of the title role.\"]\n"
     ]
    }
   ],
   "source": [
    "print(infer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1ae97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_infer_data = tokenizer(infer_data, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "161e6294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 2074, 1037, 9062, 2210, 6323, 1012, 1996, 2377, 1010, 1996, 5896, 2024, 6581, 1012, 1045, 2064, 2102, 12826, 2023, 3185, 2007, 2505, 2842, 1010, 2672, 3272, 1996, 3185, 1000, 6506, 1000, 6919, 2135, 2209, 2011, 3744, 17738, 1998, 10829, 3417, 2386, 1012, 2021, 1012, 1012, 1012, 2054, 2064, 1045, 2360, 2055, 2023, 2028, 1029, 2023, 2003, 1996, 2190, 3185, 4776, 11968, 9386, 6784, 2038, 2412, 2209, 1999, 1006, 2156, 3531, 1000, 12784, 2732, 7138, 1000, 1010, 2016, 1005, 1055, 4092, 2394, 2045, 1007, 2000, 2156, 2054, 1045, 2812, 1012, 1996, 2466, 1997, 2402, 7196, 2611, 29106, 1010, 2579, 2046, 1996, 2139, 18098, 10696, 2094, 2088, 1997, 1996, 3595, 2231, 2749, 2038, 2042, 17077, 2058, 2109, 2011, 4841, 1012, 2196, 2568, 1996, 1000, 2391, 1997, 2053, 2709, 1000, 1998, 2926, 1996, 1000, 2474, 26893, 29106, 1000, 2694, 2186, 1012, 2027, 3685, 12826, 1996, 2434, 2903, 2033, 999, 11669, 2122, 6876, 1012, 4965, 2023, 2028, 1010, 2079, 2025, 9278, 2009, 1010, 4965, 2009, 1012, 18411, 2860, 2022, 8059, 1997, 1996, 4942, 27430, 1997, 1996, 2474, 2194, 2029, 1000, 17637, 1000, 1996, 2149, 2713, 1012, 2054, 1037, 29591, 999, 2065, 2017, 2064, 2102, 3305, 2413, 1010, 2131, 1037, 9188, 2544, 1012, 2021, 2017, 1005, 2222, 9038, 2101, 1024, 1007, 102], [101, 2043, 1045, 2360, 2023, 2003, 2026, 8837, 2143, 1997, 2035, 2051, 1010, 2008, 7615, 2003, 2025, 2000, 2022, 2579, 8217, 1012, 1045, 2763, 3422, 2521, 2205, 2116, 3152, 2084, 2003, 7965, 2005, 2033, 1010, 1998, 2031, 3866, 3243, 1037, 2261, 1997, 2068, 1012, 1045, 2034, 2387, 1000, 2474, 26893, 29106, 1000, 3053, 2702, 2086, 3283, 1010, 1998, 2009, 2145, 9020, 2000, 2022, 2026, 7619, 8837, 1012, 2339, 1029, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2023, 2003, 2062, 2084, 2019, 11757, 2358, 8516, 4509, 1998, 7916, 10874, 1012, 12776, 2022, 7092, 1005, 1055, 2307, 22012, 2005, 17727, 8586, 21170, 3257, 1010, 4827, 1010, 1998, 6413, 8192, 1997, 2189, 3084, 2023, 1037, 2200, 3422, 3085, 2143, 1012, 2021, 2009, 2003, 4776, 11968, 9386, 6784, 1005, 1055, 3819, 14259, 1997, 1037, 3375, 2839, 2040, 21743, 2013, 1037, 2540, 3238, 6359, 2046, 1037, 29353, 1010, 17026, 2402, 2450, 2008, 3084, 2023, 2143, 3376, 1012, 1045, 2064, 1005, 1056, 2562, 2026, 2159, 2125, 1997, 2014, 2043, 2016, 2003, 2006, 3898, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2031, 2464, 2195, 1997, 12776, 2022, 7092, 1005, 1055, 3152, 2164, 1000, 10798, 1000, 1010, 1000, 1996, 2658, 1000, 1010, 1998, 1996, 29348, 1000, 3587, 5783, 1000, 1010, 1998, 1000, 29106, 1000, 2003, 2302, 1037, 4797, 1010, 2521, 6020, 2000, 2151, 1997, 2122, 1012, 2348, 2023, 2143, 2038, 13800, 3787, 1010, 2009, 2003, 4821, 5186, 17772, 1012, 2009, 2003, 1996, 2466, 1997, 1037, 2711, 2040, 2003, 10311, 1998, 21442, 6895, 3238, 1010, 2040, 4821, 3310, 2000, 5382, 2014, 2219, 8438, 1998, 2014, 2219, 3167, 2373, 1012, 2008, 1010, 2000, 2033, 2003, 5186, 18988, 1012, 2065, 2045, 2003, 3246, 2005, 29106, 1010, 2045, 2003, 3246, 2005, 2035, 1997, 2149, 1012, 102], [101, 1045, 2387, 2023, 3185, 2138, 1045, 2572, 1037, 4121, 5470, 1997, 1996, 2694, 2186, 1997, 1996, 2168, 2171, 4626, 6060, 4241, 14289, 2483, 1998, 9004, 4267, 1012, 1996, 3185, 2001, 2428, 2204, 1998, 1045, 2387, 2129, 1996, 2694, 2265, 2003, 2241, 2006, 1996, 3185, 1012, 1037, 2261, 4178, 1997, 1996, 2694, 2186, 2234, 3495, 2013, 1996, 3185, 1998, 2037, 14402, 2001, 6429, 1012, 2000, 2562, 2477, 2460, 1010, 2151, 5470, 1997, 1996, 3185, 2038, 2000, 3422, 1996, 2186, 1998, 2151, 5470, 1997, 1996, 2186, 2442, 2156, 1996, 2434, 29106, 1012, 102], [101, 2108, 2008, 1996, 2069, 3097, 3152, 1045, 2788, 2066, 2732, 1037, 2887, 2711, 1999, 1037, 8903, 4848, 2040, 10188, 2229, 2210, 4714, 3121, 1998, 7286, 1010, 1045, 2018, 2152, 8069, 2005, 2023, 3185, 1012, 1045, 2245, 2008, 2023, 2001, 1037, 3185, 2008, 2876, 1005, 1056, 2404, 2033, 2000, 3637, 1012, 3308, 999, 4627, 2125, 2007, 1037, 9748, 1010, 3100, 1010, 2085, 2016, 1005, 1055, 1999, 2731, 1010, 10303, 1010, 2016, 1005, 1055, 2019, 12025, 1010, 1045, 1005, 1049, 2145, 2007, 2017, 1010, 2821, 1010, 2085, 2016, 1005, 1055, 2383, 2023, 7191, 21883, 1998, 2016, 2064, 1005, 1056, 5630, 2065, 2016, 7459, 2014, 6898, 2030, 2014, 11486, 1010, 1062, 13213, 13213, 1012, 1012, 1012, 1012, 2821, 2092, 1010, 2067, 2000, 27911, 2050, 999, 102], [101, 2044, 3773, 2391, 1997, 2053, 2709, 1006, 1037, 2307, 3185, 1007, 1998, 2108, 2409, 2008, 1996, 2434, 2001, 2488, 1010, 1045, 2001, 5121, 16082, 2000, 2156, 2008, 2028, 1997, 1996, 10271, 2143, 6833, 2001, 2770, 2474, 26893, 29106, 1012, 2059, 1045, 2387, 1996, 3185, 1012, 15068, 2818, 999, 2023, 2001, 1037, 2350, 2292, 1011, 2091, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 29106, 2841, 15537, 2033, 1997, 15723, 15723, 8026, 5705, 2062, 2084, 2151, 2060, 2839, 1045, 1005, 2310, 2464, 3728, 1012, 2016, 3310, 2408, 4498, 2004, 5021, 4335, 1012, 1996, 3185, 3432, 2038, 2498, 2000, 16755, 2009, 4661, 1996, 4563, 4145, 1997, 2019, 4763, 1010, 29582, 2839, 20506, 15004, 4083, 2000, 2022, 2529, 2096, 2731, 2004, 2019, 12025, 1010, 1998, 2008, 4145, 3478, 28616, 6906, 6321, 1999, 29106, 2349, 2000, 1996, 3532, 3015, 1997, 1996, 2516, 2535, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_infer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08be9117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is just a precious little diamond. The pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I say this is my favourite film of all ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I saw this movie because I am a huge fan of th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Being that the only foreign films I usually li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After seeing Point of No Return (a great movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  predicted_class\n",
       "0  This is just a precious little diamond. The pl...                0\n",
       "1  When I say this is my favourite film of all ti...                1\n",
       "2  I saw this movie because I am a huge fan of th...                1\n",
       "3  Being that the only foreign films I usually li...                0\n",
       "4  After seeing Point of No Return (a great movie...                0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_classes = []\n",
    "\n",
    "for text in range(len(tokenized_infer_data['input_ids'])):\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(\n",
    "            tokenized_infer_data['input_ids'][text]).unsqueeze(0).to(device)\n",
    "        outputs = best_model(input_ids=input_ids)\n",
    "        logits = outputs.logits.to(device)\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_classes.append(predicted_class)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\"input_ids\": infer_data, \"predicted_class\": predicted_classes})\n",
    "\n",
    "# Display DataFrame\n",
    "display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
